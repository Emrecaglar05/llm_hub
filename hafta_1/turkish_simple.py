"""
Basit TÃ¼rkÃ§e LLM Ã–rneÄŸi (GPU destekli)
"""

from transformers import pipeline

# TÃ¼rkÃ§e destekli model
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"

print("ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e LLM yÃ¼kleniyor...")

# Pipeline oluÅŸtur
generator = pipeline(
    "text-generation",
    model=MODEL_ID,
    device_map="auto",   # GPU varsa kullanÄ±r
    max_new_tokens=50,  # maksimum 50 token Ã¼retir
    temperature=0.3      # yaratÄ±cÄ± cevaplar iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
)

# TÃ¼rkÃ§e sorular
questions = [
    "Merhaba",
    "2+2 kaÃ§ eder?",
    "Python nedir?", 
    "Yapay zeka nasÄ±l Ã§alÄ±ÅŸÄ±r?"
]

print("âœ… Model hazÄ±r! TÃ¼rkÃ§e sorular test ediliyor...\n")

for i, question in enumerate(questions, 1): # SorularÄ± sÄ±rayla iÅŸlemeye yarar enumerate
    print(f"ğŸ”¤ Soru {i}: {question}")
    
    # Qwen chat formatÄ±
    prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    #Qwen, tek seferlik metin Ã¼retiminden ziyade sohbet iÃ§in tasarlanmÄ±ÅŸ. 
    # KullanÄ±cÄ±/assistant etiketleri ile kim ne sÃ¶ylediÄŸini ayÄ±rt ediyor. 
    # Bu sayede model â€œasistanâ€ rolÃ¼nÃ¼ bilir ve cevaplarÄ± kullanÄ±cÄ± rolÃ¼ne karÄ±ÅŸmaz.
    
    try:
        response = generator(prompt)
        full_text = response[0]["generated_text"]
        
        # Sadece assistant cevabÄ±nÄ± al
        if "<|im_start|>assistant\n" in full_text:
            answer = full_text.split("<|im_start|>assistant\n")[-1]
            answer = answer.split("<|im_end|>")[0].strip()
            print(f"ğŸ¤– Cevap: {answer}")
        else:
            print(f"ğŸ¤– Cevap: {full_text}")
            
    except Exception as e:
        print(f"âŒ Hata: {e}")
    
    print("-" * 60)

print("\nğŸ‰ Test tamamlandÄ±!")
